
# ğŸ’° Finance Data Pipeline Project

A complete end-to-end data lakehouse pipeline built using AWS services including S3, Glue, Athena, and Delta Lake, with data processed using PySpark and orchestrated through Glue Jobs and Terraform.

---

## ğŸ“Œ Project Objectives

- Ingest raw finance-related data (accounts, loans, customers, transactions, payments) into an AWS S3-based Bronze layer.
- Clean, enrich, and deduplicate data using PySpark ETL jobs.
- Store transformed data in Delta Lake format in Silver and Gold layers.
- Use AWS Glue Data Catalog for table management and schema evolution.
- Enable downstream BI and ML use cases (QuickSight, SageMaker).
- Automate infrastructure using Terraform for reproducibility and scalability.

---

## ğŸ“ Project Structure

```
Finance-data-pipeline/
â”‚
â”œâ”€â”€ scripts/                     # PySpark ETL scripts for each dataset
â”‚   â”œâ”€â”€ accounts_etl.py
â”‚   â”œâ”€â”€ customers_etl.py
â”‚   â”œâ”€â”€ loans_etl.py
â”‚   â”œâ”€â”€ payments_etl.py
â”‚   â”œâ”€â”€ transactions_etl.py
â”‚
â”œâ”€â”€ terraform/                   # Infrastructure as Code (IaC)
â”‚   â”œâ”€â”€ main.tf                  # Root configuration
â”‚   â”œâ”€â”€ variables.tf             # Input variables
â”‚   â”œâ”€â”€ outputs.tf               # Output values
â”‚   â”œâ”€â”€ providers.tf             # AWS provider setup
â”‚   â”œâ”€â”€ backend.tf               # Remote state config (optional)
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ s3/                  # S3 buckets (bronze, silver, gold)
â”‚   â”‚   â”œâ”€â”€ s3_public/          # Public S3 buckets for Databricks access
â”‚   â”‚   â”œâ”€â”€ glue/               # Glue databases, crawlers
â”‚   â”‚   â””â”€â”€ iam/                # IAM roles and policies
â”‚
â”œâ”€â”€ data/                        # Synthetic data (optional for testing)
â”‚
â””â”€â”€ README.md                    # Project documentation
```

---

## ğŸ—ï¸ Infrastructure Setup

All infrastructure is managed via Terraform modules:

### Modules

| Module       | Resources Provisioned                     |
|--------------|--------------------------------------------|
| `s3`         | Bronze, Silver, and Gold S3 buckets       |
| `s3_public`  | Public Silver/Gold S3 buckets (for Databricks) |
| `glue`       | Glue Databases and Crawlers               |
| `iam`        | IAM Roles for Glue Jobs and Crawlers      |

> ğŸ” Access is restricted using least-privilege IAM roles. Public buckets explicitly allow read/write for demo purposes.

---

## ğŸ” ETL Process Overview

Each ETL script performs the following steps:

1. **Read raw data from Bronze layer (S3 or Glue Catalog)**
2. **Apply data cleaning and enrichment**
3. **Generate data lineage (e.g., ingestion timestamp, filename)**
4. **Deduplicate using window functions and SHA2 row hashes**
5. **Write to Silver layer in Delta format and register to Glue**

---

## ğŸ“Š Data Lake Architecture

```
S3 (Bronze Layer - Raw CSV)
   â”‚
   â””â”€â”€â–¶ Glue Job (PySpark) â†’ Delta Tables (Silver Layer)
                                  â”‚
                                  â””â”€â”€â–¶ Gold Layer (Aggregated Views, KPIs)

Other Integrations:
- ğŸ§  ML Models (SageMaker)
- ğŸ“ˆ Dashboards (QuickSight)
```

---

## ğŸ”’ Security & Compliance

- **PII Tagging**: Sensitive fields (e.g., email, phone, dob) are tagged during Silver layer processing.
- **Access Control**:
  - S3 bucket policies restrict unauthorized access.
  - IAM roles created with scoped permissions.
- **Logging**: Glue Jobs and Crawlers write logs to CloudWatch.

---

## ğŸš€ Getting Started

### ğŸ§± Deploy Infra

```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### ğŸ§ª Run ETL Jobs

Upload your PySpark ETL scripts to `s3://<glue-script-bucket>/scripts/` and trigger the jobs manually or using the AWS Console.

---

## ğŸ“š Future Enhancements

- Add CI/CD for Glue Jobs (using CodePipeline or Jenkins)
- Add partitioning strategy for performance
- Automate Gold layer aggregations
- Integrate with Amazon SageMaker for ML models

---

## ğŸ§‘â€ğŸ’» Author

**[@donofthegame](https://github.com/donofthegame)** â€“ Building scalable data platforms with AWS and open-source tools.

---

## ğŸ“ License

This project is licensed under the MIT License.
